---
layout: default
title: Progetti - Cyber Security e Protezione dei Dati - Anno Accademico 2025-2026 
description: Prof.ssa Federica Paci
---
Questa pagina è dedicata ai progetti per sostenere l'esame del corso di Cyber sicurezza e Protezione dei Dati. I progetti possono essere svolti in gruppo costituito al massimo da 2 studenti.

## Analisi delle Allucinazioni nei Modelli Linguistici di Grande Scala (LLM) e Valutazione delle Tecniche di Rilevamento per la Sicurezza e la Protezione dei Dati

L’obiettivo del progetto è investigare in modo sistematico le diverse tipologie di allucinazioni generate dai modelli linguistici di grande scala (LLM), con particolare attenzione agli impatti sulla cybersicurezza, sulla privacy e sulla protezione dei dati personali.

Le allucinazioni, ovvero le risposte false, inventate o fuorvianti generate un modello, rappresentano una criticità rilevante nei contesti in cui le informazioni devono essere accurate, verificabili e sicure.

Il progetto mira a comprendere tali fenomeni, classificarli e analizzare le tecniche più recenti per il loro rilevamento e mitigazione.

**Obiettivi principali**

- Classificazione delle allucinazioni negli LLM

- Analisi delle cause tecniche

- Valutazione delle tecniche di rilevamento e mitigazione

Per sostenere l'esame gli studenti devono consegnare un report tecnico con classificazione delle allucinazioni osservate, analisi delle cause e panoramica delle tecniche di rilevamento e mitigazione.

##Progettazione e Analisi di un Attacco di Indirect Prompt Injection contro Modelli Linguistici##

Il progetto ha l’obiettivo di progettare, implementare e valutare un attacco di Indirect Prompt Injection (IPI), una tecnica emergente che mira a manipolare il comportamento di un modello linguistico attraverso istruzioni nascoste all'interno di contenuti apparentemente innocui provenienti da fonti esterne.

A differenza della direct prompt injection, che richiede un input malevolo fornito direttamente all’interfaccia del modello, l’IPI sfrutta contenuti recuperati, incorporati o elaborati automaticamente dall’LLM (come testi presenti in pagine web, file, email, dataset o API esterne) per indurre il modello a violare le istruzioni originali, eseguire azioni non autorizzate o generare output non sicuri.

Il progetto esplorerà le vulnerabilità dei sistemi basati su LLM che integrano contenuti esterni e proporrà tecniche per dimostrare e valutare l’efficacia dell’attacco.

**Obiettivi del progetto**

- Analizzare il funzionamento degli attacchi di Indirect Prompt Injection

- Comprendere i meccanismi che consentono a contenuti esterni di influenzare il prompt interno.

- Progettare un contenuto malevolo nascosto

- Creare testi, metadati o markup contenenti istruzioni furtive.

- Incorporare il payload all’interno di materiali recuperati automaticamente dal modello (es. descrizioni, commenti HTML, note nascoste, contenuti offuscati).

- Costruire un ambiente di test

- Configurare una semplice applicazione basata su  LLM che recuperi automaticamente contenuti esterni (es. web scraping controllato, API simulate, file caricati dall’utente).

- Implementare pipeline che dimostrino come l’attacco si innesca senza intervento diretto dell’utente finale.

- Analizzare il comportamento dell’LLM dopo l'iniezione (es. deviazione degli output, fuga di dati, esecuzione di comandi non previsti).

- Studiare possibili contromisure

Per sostenere l'esame orale gli studenti dovranno: 

1) Implementare un proof-of-concept funzionante di un attacco di indirect prompt injection.

2) Effettuare un’analisi dettagliata delle condizioni necessarie all’esecuzione dell’attacco.

3) Suggerire linee guida e best practice di mitigazione per sviluppatori di applicazioni basate su LLM.

4) Preparare presentazione con dimostrazione controllata dell’attacco
